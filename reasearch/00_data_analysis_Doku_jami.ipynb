{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import dataframe_image as dfi\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import plotly.io as pio\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMI Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordnerpfad zu den CSV-Dateien\n",
    "csv_folder = 'data/Json_Dateien'\n",
    "\n",
    "# Leere Liste zum Speichern der DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Alle CSV-Dateien im Ordner einlesen\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_file = os.path.join(csv_folder, filename)  # Vollständiger Pfad zur CSV-Datei\n",
    "        df = pd.read_csv(csv_file)  # Einlesen der CSV-Datei\n",
    "        dataframes.append(df)  # Hinzufügen des DataFrames zur Liste\n",
    "\n",
    "# Alle DataFrames zu einem einzigen DataFrame zusammenführen\n",
    "df_remit = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_null = df_remit.isnull().sum()\n",
    "\n",
    "df_energy_null = pd.DataFrame(energy_null).reset_index()\n",
    "df_energy_null.columns = ['Feature', 'Null Count'] \n",
    "\n",
    "\n",
    "\n",
    "display(df_energy_null)\n",
    "# df_energy_null = pd.DataFrame(df_energy_null).reset_index()\n",
    "# df_energy_null.columns = ['Feature', 'Null Count'] \n",
    "\n",
    "\n",
    "\n",
    "# display(df_energy_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "      \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "     \n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        \n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "       \n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_20200920_20231027  = pd.read_csv(r\"data\\Energy_Data_20200920_20231027.csv\")\n",
    "energy_20200920_20240118  = pd.read_csv(r\"data\\Energy_Data_20200920_20240118.csv\")\n",
    "energy_20240119_20240519  = pd.read_csv(r\"data\\Energy_Data_20240119_20240519.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.concat([energy_20200920_20240118, energy_20240119_20240519], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_h3 = energy.head(3).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = energy[energy.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_null = energy.isnull().sum()\n",
    "\n",
    "df_energy_null = pd.DataFrame(energy_null).reset_index()\n",
    "df_energy_null.columns = ['Feature', 'Null Count'] \n",
    "\n",
    "\n",
    "\n",
    "display(df_energy_null)\n",
    "\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "      \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "     \n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        \n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "       \n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = energy.min()\n",
    "max_values = energy.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "\n",
    "merged_df_standard = pd.merge(df_energy_null, min_max_df, on='Feature', how='inner')\n",
    "\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'dtm']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(['dtm'])\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataframes = {\n",
    "    'df_energy_data_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_energy_cols = energy.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_energy_cols.columns)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(data=num_energy_cols, x=num_energy_cols.columns[i])\n",
    "    plt.title(num_energy_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_energy_cols = energy.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_energy_cols.columns)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(data=num_energy_cols, x=num_energy_cols.columns[i])\n",
    "    plt.title(num_energy_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = energy.select_dtypes(include=\"number\").corr()\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DWD hornsea 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hornsea(dataset: xr.Dataset) -> pd.DataFrame:\n",
    "    df = (\n",
    "        dataset\n",
    "        .mean(dim=[\"latitude\", \"longitude\"])\n",
    "        .to_dataframe()\n",
    "        .reset_index()\n",
    "    ).rename(columns={\"ref_datetime\": \"reference_time\", \"valid_datetime\": \"valid_time\"})\n",
    "    df = (\n",
    "        df.assign(\n",
    "            reference_time=df[\"reference_time\"].dt.tz_localize(\"UTC\"),\n",
    "            hours_after=df[\"valid_time\"],\n",
    "            valid_time=(df[\"reference_time\"] + pd.to_timedelta(df[\"valid_time\"], unit=\"hours\")).dt.tz_localize(\"UTC\")\n",
    "        )\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwd_hornsea_1 = load_hornsea(xr.load_dataset(r\"data\\dwd_icon_eu_hornsea_1_20200920_20231027.nc\", engine=\"h5netcdf\"))\n",
    "dwd_hornsea_2 = load_hornsea(xr.load_dataset(r\"data\\dwd_icon_eu_hornsea_1_20231027_20240108.nc\", engine=\"h5netcdf\"))\n",
    "dwd_hornsea_3 = load_hornsea(xr.load_dataset(r\"data\\dwd_icon_eu_hornsea_1_20240108_20240129.nc\", engine=\"h5netcdf\"))\n",
    "dwd_hornsea_4 = load_hornsea(xr.load_dataset(r\"data\\dwd_icon_eu_hornsea_1_20240129_20240519.nc\", engine=\"h5netcdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd = (pd.concat([dwd_hornsea_1, dwd_hornsea_2, dwd_hornsea_3, dwd_hornsea_4])\n",
    "              .sort_values([\"reference_time\", \"valid_time\"])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd_h3 = df_hornsea_dwd.head(3).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df_hornsea_dwd[df_hornsea_dwd.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd[df_hornsea_dwd['RelativeHumidity']>100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd_null = df_hornsea_dwd.isnull().sum()\n",
    "df_hornsea_dwd_null\n",
    "\n",
    "df_hornsea_dwd_null = pd.DataFrame(df_hornsea_dwd_null).reset_index()\n",
    "df_hornsea_dwd_null.columns = ['Feature', 'Null Count'] \n",
    "\n",
    "\n",
    "\n",
    "display(df_hornsea_dwd_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "      \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "     \n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        \n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "       \n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = df_hornsea_dwd.min()\n",
    "max_values = df_hornsea_dwd.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "\n",
    "merged_df_standard = pd.merge(df_hornsea_dwd_null, min_max_df, on='Feature', how='inner')\n",
    "\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'reference_time','valid_time']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(expected_columns)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "\n",
    "\n",
    "\n",
    "dataframes = {\n",
    "    'df_dwd_hornsea_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dwd_hornsea_cols = df_hornsea_dwd.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_dwd_hornsea_cols.columns)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(data=num_dwd_hornsea_cols, x=num_dwd_hornsea_cols.columns[i])\n",
    "    plt.title(num_dwd_hornsea_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dwd_hornsea_cols = df_hornsea_dwd.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_dwd_hornsea_cols.columns)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(data=num_dwd_hornsea_cols, x=num_dwd_hornsea_cols.columns[i])\n",
    "    plt.title(num_dwd_hornsea_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_hornsea_dwd.select_dtypes(include=\"number\").corr()\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pes10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_dwd(df):\n",
    "    # Konvertiert ein xarray Dataset in einen DataFrame und setzt den Index zurück\n",
    "    df = df.to_dataframe().reset_index()\n",
    "    # Lokalisiert 'ref_datetime' auf UTC\n",
    "    df.ref_datetime = df.ref_datetime.dt.tz_localize(\"UTC\")\n",
    "    # Berechnet 'valid_datetime' basierend auf 'ref_datetime' und einer Zeitspanne in Stunden\n",
    "    df.valid_datetime = df.ref_datetime + df.valid_datetime * pd.Timedelta(1, \"h\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10_1 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_pes10_20200920_20231027.nc\")).rename(\n",
    "        # {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "df_dwd_pes10_2 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_pes10_20231027_20240108.nc\")).rename(\n",
    "         {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "df_dwd_pes10_3 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/ncep_gfs_pes10_20240108_20240129.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "df_dwd_pes10_4 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_pes10_20240129_20240519.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "\n",
    "df_dwd_pes10 = (pd.concat([df_dwd_pes10_1, df_dwd_pes10_2, df_dwd_pes10_3, df_dwd_pes10_4])\n",
    "              .sort_values([\"ref_datetime\", \"valid_datetime\"])\n",
    "              .reset_index(drop=True))\n",
    "df_dwd_pes10.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10_h3 = df_dwd_pes10.head(3).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df_dwd_pes10[df_dwd_pes10.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10_null = df_dwd_pes10.isnull().sum()\n",
    "df_dwd_pes10_null\n",
    "# In ein DataFrame umwandeln\n",
    "df_dwd_pes10_null = pd.DataFrame(df_dwd_pes10_null).reset_index()\n",
    "df_dwd_pes10_null.columns = ['Feature', 'Null Count']  # Spalten umbenennen\n",
    "\n",
    "\n",
    "# Ausgabe des neuen DataFrames\n",
    "display(df_dwd_pes10_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        # Überprüfen, ob df ein DataFrame ist\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "        # Dateiname für den DataFrame\n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "        # Styling-Optionen definieren\n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        # Vertikale und horizontale Linien für jede Zelle hinzufügen\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Dateipfad und -namen kombinieren\n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "        # DataFrame als Bild speichern mit dfi\n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = df_dwd_pes10.min()\n",
    "max_values = df_dwd_pes10.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "# df_dwd_pes10_null.merge(min_max_df,on = 'Feature', how='inner')\n",
    "merged_df_standard = pd.merge(df_dwd_pes10_null, min_max_df, on='Feature', how='inner')\n",
    "\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'ref_datetime','valid_datetime']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(expected_columns)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "\n",
    "# Korrigierter DataFrame-Namen\n",
    "dataframes = {\n",
    "    'df_dwd_pes10_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'C:\\Users\\Michael Jäckle\\Desktop\\00_Inbox\\Now\\00_Bilder_Doku')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_csv(df):\n",
    "    # Wandelt die Spalte 'dtm' in datetime-Objekte um\n",
    "    df.dtm = pd.to_datetime(df.dtm)\n",
    "    return df\n",
    "\n",
    "def pre_dwd(df):\n",
    "    # Konvertiert ein xarray Dataset in einen DataFrame und setzt den Index zurück\n",
    "    df = df.to_dataframe().reset_index()\n",
    "    # Lokalisiert 'ref_datetime' auf UTC\n",
    "    df.ref_datetime = df.ref_datetime.dt.tz_localize(\"UTC\")\n",
    "    # Berechnet 'valid_datetime' basierend auf 'ref_datetime' und einer Zeitspanne in Stunden\n",
    "    df.valid_datetime = df.ref_datetime + df.valid_datetime * pd.Timedelta(1, \"h\")\n",
    "    return df\n",
    "\n",
    "def pre_ncep(df):\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand_1 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_demand_20200920_20231027.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "df_dwd_demand_2 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_demand_20231027_20240108.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "df_dwd_demand_3 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_demand_20240108_20240129.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "df_dwd_demand_4 = pre_dwd(\n",
    "    (dataset := xr.load_dataset(\"data/dwd_icon_eu_demand_20240129_20240519.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset}\n",
    "    )\n",
    ")\n",
    "\n",
    "df_dwd_demand = (pd.concat([df_dwd_demand_1, df_dwd_demand_2, df_dwd_demand_3, df_dwd_demand_4])\n",
    "              .sort_values([\"ref_datetime\", \"valid_datetime\"])\n",
    "              .reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand_h3 = df_dwd_demand.head(3).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df_dwd_demand[df_dwd_demand.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand_null = df_dwd_demand.isnull().sum()\n",
    "df_dwd_demand_null\n",
    "# In ein DataFrame umwandeln\n",
    "df_dwd_demand_null = pd.DataFrame(df_dwd_demand_null).reset_index()\n",
    "df_dwd_demand_null.columns = ['Feature', 'Null Count']  # Spalten umbenennen\n",
    "\n",
    "\n",
    "# Ausgabe des neuen DataFrames\n",
    "display(df_dwd_demand_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        # Überprüfen, ob df ein DataFrame ist\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "        # Dateiname für den DataFrame\n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "        # Styling-Optionen definieren\n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        # Vertikale und horizontale Linien für jede Zelle hinzufügen\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Dateipfad und -namen kombinieren\n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "        # DataFrame als Bild speichern mit dfi\n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = df_dwd_demand.min()\n",
    "max_values = df_dwd_demand.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "# df_dwd_pes10_null.merge(min_max_df,on = 'Feature', how='inner')\n",
    "merged_df_standard = pd.merge(df_dwd_demand_null, min_max_df, on='Feature', how='inner')\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'ref_datetime','valid_datetime']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(expected_columns)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "\n",
    "# Korrigierter DataFrame-Namen\n",
    "dataframes = {\n",
    "    'df_dwd_demand_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100/ 3441186*76615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCEP Hornsea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncep_hornsea_1 = load_hornsea(xr.load_dataset(r\"data\\ncep_gfs_hornsea_1_20200920_20231027.nc\", engine=\"h5netcdf\"))\n",
    "ncep_hornsea_2 = load_hornsea(xr.load_dataset(r\"data\\ncep_gfs_hornsea_1_20231027_20240108.nc\", engine=\"h5netcdf\"))\n",
    "ncep_hornsea_3 = load_hornsea(xr.load_dataset(r\"data\\ncep_gfs_hornsea_1_20240108_20240129.nc\", engine=\"h5netcdf\"))\n",
    "ncep_hornsea_4 = load_hornsea(xr.load_dataset(r\"data\\ncep_gfs_hornsea_1_20240129_20240519.nc\", engine=\"h5netcdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_ncep = (pd.concat([ncep_hornsea_1, ncep_hornsea_2, ncep_hornsea_3, ncep_hornsea_4])\n",
    "              .sort_values([\"reference_time\", \"valid_time\"])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_ncep_h3 = df_hornsea_ncep.head(3).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_ncep.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hornsea_ncep_cols = df_hornsea_ncep.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_hornsea_ncep_cols.columns)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(data=num_hornsea_ncep_cols, x=num_hornsea_ncep_cols.columns[i])\n",
    "    plt.title(num_hornsea_ncep_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hornsea_ncep_cols = df_hornsea_ncep.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_hornsea_ncep_cols.columns)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(data=num_hornsea_ncep_cols, x=num_hornsea_ncep_cols.columns[i])\n",
    "    plt.title(num_hornsea_ncep_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_hornsea_h3 = df_hornsea_ncep.head(3).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_ncep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df_hornsea_ncep[df_hornsea_ncep.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_ncep_null = df_hornsea_ncep.isnull().sum()\n",
    "df_hornsea_ncep_null\n",
    "# In ein DataFrame umwandeln\n",
    "df_hornsea_ncep_null = pd.DataFrame(df_hornsea_ncep_null).reset_index()\n",
    "df_hornsea_ncep_null.columns = ['Feature', 'Null Count']  # Spalten umbenennen\n",
    "\n",
    "\n",
    "# Ausgabe des neuen DataFrames\n",
    "display(df_hornsea_ncep_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        # Überprüfen, ob df ein DataFrame ist\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "        # Dateiname für den DataFrame\n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "        # Styling-Optionen definieren\n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        # Vertikale und horizontale Linien für jede Zelle hinzufügen\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Dateipfad und -namen kombinieren\n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "        # DataFrame als Bild speichern mit dfi\n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = df_hornsea_ncep.min()\n",
    "max_values = df_hornsea_ncep.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "# df_dwd_pes10_null.merge(min_max_df,on = 'Feature', how='inner')\n",
    "merged_df_standard = pd.merge(df_hornsea_ncep_null, min_max_df, on='Feature', how='inner')\n",
    "\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'reference_time','valid_time']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(expected_columns)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "# Korrigierter DataFrame-Namen\n",
    "dataframes = {\n",
    "    'df_ncep_hornsea_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_hornsea_ncep.select_dtypes(include=\"number\").corr()\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ncep_gfs_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_ncep(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_demand_1 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_demand_20200920_20231027.nc\"))\n",
    "df_ncep_demand_1 = df_ncep_demand_1.to_dataframe().reset_index()\n",
    "df_ncep_demand_2 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_demand_20231027_20240108.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset})\n",
    "df_ncep_demand_2 = df_ncep_demand_2.to_dataframe().reset_index()\n",
    "df_ncep_demand_3 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_demand_20240108_20240129.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset})\n",
    "df_ncep_demand_3 = df_ncep_demand_3.to_dataframe().reset_index()\n",
    "df_ncep_demand_4 = pre_ncep(xr.open_dataset(\"data/dwd_icon_eu_demand_20240129_20240519.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset})\n",
    "df_ncep_demand_4 = df_ncep_demand_4.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_demand = (pd.concat([df_ncep_demand_1, df_ncep_demand_2, df_ncep_demand_3, df_ncep_demand_4])\n",
    "              .sort_values([\"ref_datetime\", \"valid_datetime\"])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_demand_h3 = df_ncep_demand.head(3).round(2)\n",
    "df_ncep_demand_h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_demand.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df_ncep_demand[df_ncep_demand.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_demand_null = df_ncep_demand.isnull().sum()\n",
    "df_ncep_demand_null\n",
    "# In ein DataFrame umwandeln\n",
    "df_ncep_demand_null = pd.DataFrame(df_ncep_demand_null).reset_index()\n",
    "df_ncep_demand_null.columns = ['Feature', 'Null Count']  # Spalten umbenennen\n",
    "\n",
    "\n",
    "# Ausgabe des neuen DataFrames\n",
    "display(df_ncep_demand_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        # Überprüfen, ob df ein DataFrame ist\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "        # Dateiname für den DataFrame\n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "        # Styling-Optionen definieren\n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        # Vertikale und horizontale Linien für jede Zelle hinzufügen\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Dateipfad und -namen kombinieren\n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "        # DataFrame als Bild speichern mit dfi\n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = df_ncep_demand.min()\n",
    "max_values = df_ncep_demand.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "# df_dwd_pes10_null.merge(min_max_df,on = 'Feature', how='inner')\n",
    "merged_df_standard = pd.merge(df_ncep_demand_null, min_max_df, on='Feature', how='inner')\n",
    "\n",
    "\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'ref_datetime','valid_datetime']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(expected_columns)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "\n",
    "\n",
    "# Korrigierter DataFrame-Namen\n",
    "dataframes = {\n",
    "    'df_ncep_demand_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_ncep_pes10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_ncep(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_pes10_1 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_pes10_20200920_20231027.nc\"))\n",
    "df_ncep_pes10_1 = df_ncep_pes10_1.to_dataframe().reset_index()\n",
    "df_ncep_pes10_2 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_pes10_20231027_20240108.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset})\n",
    "df_ncep_pes10_2 = df_ncep_pes10_2.to_dataframe().reset_index()\n",
    "df_ncep_pes10_3 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_pes10_20240108_20240129.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset})\n",
    "df_ncep_pes10_3 = df_ncep_pes10_3.to_dataframe().reset_index()\n",
    "df_ncep_pes10_4 = pre_ncep(xr.open_dataset(\"data/ncep_gfs_pes10_20240129_20240519.nc\")).rename(\n",
    "        {old: new for old, new in {'reference_time': 'ref_datetime', 'valid_time': 'valid_datetime'}.items() if old in dataset})\n",
    "df_ncep_pes10_4 = df_ncep_pes10_4.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_pes10 = (pd.concat([df_ncep_pes10_1, df_ncep_pes10_2, df_ncep_pes10_3, df_ncep_pes10_4])\n",
    "              .sort_values([\"ref_datetime\", \"valid_datetime\"])\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_pes10_h3 = df_ncep_pes10.head(3).round(2)\n",
    "df_ncep_pes10.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_pes10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplikate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = df_ncep_pes10[df_ncep_pes10.duplicated()]\n",
    "\n",
    "print(\"Duplikate:\")\n",
    "print(dupes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_pes10_null = df_ncep_pes10.isnull().sum()\n",
    "df_ncep_pes10_null\n",
    "# In ein DataFrame umwandeln\n",
    "df_ncep_pes10_null = pd.DataFrame(df_ncep_pes10_null).reset_index()\n",
    "df_ncep_pes10_null.columns = ['Feature', 'Null Count']  # Spalten umbenennen\n",
    "\n",
    "\n",
    "# Ausgabe des neuen DataFrames\n",
    "display(df_ncep_pes10_null)\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        # Überprüfen, ob df ein DataFrame ist\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "        # Dateiname für den DataFrame\n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "        # Styling-Optionen definieren\n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "\n",
    "        # Vertikale und horizontale Linien für jede Zelle hinzufügen\n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        # Dateipfad und -namen kombinieren\n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "        # DataFrame als Bild speichern mit dfi\n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "min_values = df_ncep_pes10.min()\n",
    "max_values = df_ncep_pes10.max()\n",
    "\n",
    "min_max_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values})\n",
    "display(min_max_df)\n",
    "# df_dwd_pes10_null.merge(min_max_df,on = 'Feature', how='inner')\n",
    "merged_df_standard = pd.merge(df_ncep_pes10_null, min_max_df, on='Feature', how='inner')\n",
    "\n",
    "\n",
    "merged_df_standard = merged_df_standard.T\n",
    "merged_df_standard.columns = merged_df_standard.iloc[0]\n",
    "merged_df_standard = merged_df_standard[1:]\n",
    "\n",
    "expected_columns = ['Feature', 'ref_datetime','valid_datetime']\n",
    "existing_columns = [col for col in expected_columns if col in merged_df_standard.columns]\n",
    "\n",
    "merged_df_standard = merged_df_standard[existing_columns + [col for col in merged_df_standard.columns if col not in existing_columns]]\n",
    "\n",
    "cols_to_convert = merged_df_standard.columns.difference(expected_columns)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].astype(float)\n",
    "merged_df_standard[cols_to_convert] = merged_df_standard[cols_to_convert].round(2)\n",
    "\n",
    "\n",
    "# Korrigierter DataFrame-Namen\n",
    "dataframes = {\n",
    "    'df_ncep_pes10_standard': merged_df_standard,  \n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "display(merged_df_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100/22233420*104180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DWD Solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwd_solar = pd.read_parquet(r\"data\\dwd_solar_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dwd_solar_cols = dwd_solar.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_dwd_solar_cols.columns)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(data=num_dwd_solar_cols, x=num_dwd_solar_cols.columns[i])\n",
    "    plt.title(num_dwd_solar_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dwd_solar_cols = dwd_solar.select_dtypes(include=\"number\")\n",
    "plt.figure(figsize= (12, 12))\n",
    "for i in range(len(num_dwd_solar_cols.columns)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(data=num_dwd_solar_cols, x=num_dwd_solar_cols.columns[i])\n",
    "    plt.title(num_dwd_solar_cols.columns[i])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dwd_solar.select_dtypes(include=\"number\").corr()\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordnerpfad zu den CSV-Dateien\n",
    "csv_folder = 'data/Json_Dateien/'\n",
    "\n",
    "# Leere Liste zum Speichern der DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Alle CSV-Dateien im Ordner einlesen\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_file = os.path.join(csv_folder, filename)  # Vollständiger Pfad zur CSV-Datei\n",
    "        df = pd.read_csv(csv_file)  # Einlesen der CSV-Datei\n",
    "        dataframes.append(df)  # Hinzufügen des DataFrames zur Liste\n",
    "\n",
    "# Alle DataFrames zu einem einzigen DataFrame zusammenführen\n",
    "df_remit = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Konvertiere die Spalten 'publishTime' und 'createdTime' in Datetime-Objekte\n",
    "df_remit['publishTime'] = pd.to_datetime(df_remit['publishTime'])\n",
    "df_remit['createdTime'] = pd.to_datetime(df_remit['createdTime'])\n",
    "\n",
    "# Versuche, 'eventStatus' und 'eventStartTime' in Datetime-Objekte zu konvertieren; ungültige Werte werden in NaT umgewandelt\n",
    "df_remit['eventStatus'] = pd.to_datetime(df_remit['eventStatus'], errors='coerce')\n",
    "df_remit['eventStartTime'] = pd.to_datetime(df_remit['eventStartTime'], errors='coerce')\n",
    "\n",
    "start_date = '2020-09-20'\n",
    "end_date = '2024-05-24'\n",
    "\n",
    "# DataFrame filtern für den definierten Zeitraum\n",
    "df_remit_filter = df_remit[(df_remit['createdTime'] >= start_date) & (df_remit['createdTime'] <= end_date)]\n",
    "df_remit_h3 = df_remit_filter.head(3).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit_h3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit_h3_1 = df_remit_h3.filter(['id', 'dataset', 'mrid', 'revisionNumber', 'publishTime', 'createdTime', 'eventType','unavailabilityType', \n",
    "          ])\n",
    "\n",
    "df_remit_h3_2 = df_remit_h3.filter(['id','participantId', 'registrationCode', 'assetId', 'assetType',\n",
    "       'affectedUnit', 'affectedUnitEIC', 'affectedArea', 'biddingZone', 'normalCapacity', 'availableCapacity',\n",
    "       'unavailableCapacity',])\n",
    "df_remit_h3_3 = df_remit_h3.filter(['id',\n",
    "       'unavailableCapacity', 'eventStatus', 'eventStartTime', 'eventEndTime',\n",
    "       'cause', ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit_h3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit_h3_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erstellung der Standertwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "      \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "\n",
    "      \n",
    "        png_filename = f\"{df_name}.png\"\n",
    "\n",
    "     \n",
    "        styled_df = (df.style\n",
    "            .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "            .set_table_styles(\n",
    "                [{'selector': 'thead th', \n",
    "                  'props': [('background-color', 'darkblue'), \n",
    "                            ('color', 'white'), \n",
    "                            ('font-weight', 'bold'), \n",
    "                            ('border', '1px solid black')]}],\n",
    "                axis=0\n",
    "            )\n",
    "            .set_properties(**{\n",
    "                'border': '1px solid black',\n",
    "                'text-align': 'center',\n",
    "                'font-size': '14px'\n",
    "            })\n",
    "            .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "        )\n",
    "\n",
    "        float_columns = df.select_dtypes(include=['float']).columns\n",
    "        format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "        styled_df = df.style.format(format_dict)\n",
    "        \n",
    "        styled_df.set_table_styles(\n",
    "            [{'selector': 'td, th', \n",
    "              'props': [('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        \n",
    "        png_full_path = f\"{path}/{png_filename}\"\n",
    "\n",
    "       \n",
    "        dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataframes = {\n",
    "    'df_energy_h3': energy_h3,  \n",
    "    'df_dwd_hornsea_h3': df_hornsea_dwd_h3,  \n",
    "    'df_dwd_pes10_h3': df_dwd_pes10_h3,  \n",
    "    'df_dwd_demand_h3': df_dwd_demand_h3,  \n",
    "    'df_ncep_hornsea_h3': df_ncep_hornsea_h3,  \n",
    "    'df_ncep_pes10_h3': df_ncep_pes10_h3,  \n",
    "    'df_ncep_demand_h3': df_ncep_demand_h3,  \n",
    "    'df_remit_h3_1':df_remit_h3_1,\n",
    "    'df_remit_h3_2':df_remit_h3_2,\n",
    "    'df_remit_h3_3':df_remit_h3_3,\n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schluss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz = dwd_solar.loc[\"2022-11-15\": \"2023-01-15\"]\n",
    "px.line(df_viz, x=df_viz.index, y=\"SolarDownwardRadiation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"artifacts\\prepared_data\\merged_data.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(r\"artifacts\\prepared_data\\merged_data.parquet\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = WindroseAxes.from_ax()\n",
    "ax.bar(data['WindDirection:100'], data['WindSpeed:100'], normed=True, opening=0.8, edgecolor='white')\n",
    "ax.set_legend()\n",
    "plt.title(\"Wind Direction (°) VS Wind Speed (m/s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Winddaten Ausschnitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_hornsea_dwd, x=\"valid_time\", y=[\"WindSpeed\", \"WindSpeed:100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_hornsea_dwd\n",
    "df_2 =df_hornsea_ncep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Annahme: num_dwd_hornsea_cols enthält die numerischen Spalten von df_hornsea_dwd\n",
    "num_dwd_hornsea_cols = df_hornsea_dwd.select_dtypes(include=\"number\")\n",
    "\n",
    "# Erzeuge das 3x3 Raster für die Subplots\n",
    "fig = sp.make_subplots(rows=3, cols=3, subplot_titles=num_dwd_hornsea_cols.columns)\n",
    "\n",
    "# Schleife durch jede Spalte und füge ein Histogramm zu den Subplots hinzu\n",
    "for i, column in enumerate(num_dwd_hornsea_cols.columns):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    hist = px.histogram(num_dwd_hornsea_cols, x=column)\n",
    "    fig.add_trace(hist['data'][0], row=row, col=col)\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.update_layout(height=800, width=800, title_text=\"Histogramme der numerischen Spalten\", showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_filter = 2023\n",
    "df_dwd_pes10_2023 = df_dwd_pes10[df_dwd_pes10['ref_datetime'].dt.year == year_to_filter]\n",
    "df_ncep_pes10_2023 = df_ncep_pes10[df_ncep_pes10['ref_datetime'].dt.year == year_to_filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Numerische Spalten in beiden DataFrames auswählen\n",
    "num_dwd_hornsea_cols_1 = df_ncep_pes10_2023.select_dtypes(include=\"number\")\n",
    "num_dwd_hornsea_cols_2 = df_dwd_pes10_2023.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in num_dwd_hornsea_cols_1.columns.intersection(num_dwd_hornsea_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    hist_1 = go.Histogram(x=num_dwd_hornsea_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "    fig.add_trace(hist_1, row=row, col=col)\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    hist_2 = go.Histogram(x=num_dwd_hornsea_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "    fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1200,\n",
    "    title_text=\"Histogramme der numerischen Spalten von DWD pes10 und NCEP pes10\",\n",
    "    showlegend=True,\n",
    "    margin=dict(t=50, b=20, l=20, r=20)\n",
    ")\n",
    "\n",
    "# Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "pio.write_html(fig, 'historgramm.html', auto_open=True)  # Temporäre HTML-Datei erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# richtig\n",
    "year_to_filter = 2023\n",
    "df_dwd_pes10_year = df_dwd_pes10[df_dwd_pes10['ref_datetime'].dt.year == year_to_filter]\n",
    "df_ncep_pes10_year = df_ncep_pes10[df_ncep_pes10['ref_datetime'].dt.year == year_to_filter]\n",
    "\n",
    "\n",
    "\n",
    "# Numerische Spalten in beiden DataFrames auswählen\n",
    "num_dwd_hornsea_cols_1 = df_ncep_pes10_year.select_dtypes(include=\"number\")\n",
    "num_dwd_hornsea_cols_2 = df_dwd_pes10_year.select_dtypes(include=\"number\")\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in num_dwd_hornsea_cols_1.columns.intersection(num_dwd_hornsea_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    hist_1 = go.Histogram(x=num_dwd_hornsea_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "    fig.add_trace(hist_1, row=row, col=col)\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    hist_2 = go.Histogram(x=num_dwd_hornsea_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "    fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1200,\n",
    "    title_text=f\"Histogramme der numerischen Spalten von DWD honrsea und honrsea pes10 von {year_to_filter}\",\n",
    "    showlegend=True,\n",
    "    margin=dict(t=50, b=20, l=20, r=20)\n",
    ")\n",
    "\n",
    "# Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "pio.write_html(fig, r'historgramm_dwd_pes10_vs_ncep_pes10_{year_to_filter}.html', auto_open=True)  # Temporäre HTML-Datei erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fertig\n",
    "\n",
    "# Numerische Spalten in beiden DataFrames auswählen\n",
    "num_dwd_hornsea_cols_1 = df_hornsea_ncep.select_dtypes(include=\"number\")\n",
    "num_dwd_hornsea_cols_2 = df_hornsea_dwd.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in num_dwd_hornsea_cols_1.columns.intersection(num_dwd_hornsea_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    hist_1 = go.Histogram(x=num_dwd_hornsea_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "    fig.add_trace(hist_1, row=row, col=col)\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    hist_2 = go.Histogram(x=num_dwd_hornsea_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "    fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1200,\n",
    "    title_text=f\"Histogramme der numerischen Spalten von DWD honrsea und NCEP honrsea\",\n",
    "    showlegend=True,\n",
    "    margin=dict(t=50, b=20, l=20, r=20)\n",
    ")\n",
    "\n",
    "# Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "pio.write_html(fig, f'historgramm_dwd_hornsea_vs_ncep_honrsea.html', auto_open=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Liste der Jahre, die gefiltert werden sollen\n",
    "years_to_filter = [2021, 2022,2023,2024]\n",
    "\n",
    "# Schleife durch jedes Jahr\n",
    "for year_to_filter in years_to_filter:\n",
    "    df_dwd_pes10_year = df_dwd_pes10[df_dwd_pes10['ref_datetime'].dt.year == year_to_filter]\n",
    "    df_ncep_pes10_year = df_ncep_pes10[df_ncep_pes10['ref_datetime'].dt.year == year_to_filter]\n",
    "\n",
    "\n",
    "    # Numerische Spalten in beiden DataFrames auswählen\n",
    "    num_dwd_cols_1 = df_dwd_pes10_year.select_dtypes(include=\"number\")\n",
    "    num_dwd_cols_2 = df_ncep_pes10_year.select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "    # Spalten definieren, die ausgeschlossen werden sollen\n",
    "    exclude_columns = [\"hours_after\"]\n",
    "\n",
    "    # Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "    common_columns = [col for col in num_dwd_cols_1.columns.intersection(num_dwd_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "    # Erzeuge das 2x3 Raster für die Subplots\n",
    "    fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "    # Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "    for i, column in enumerate(common_columns[:6]):\n",
    "        row = (i // 3) + 1\n",
    "        col = (i % 3) + 1\n",
    "        \n",
    "        # Histogramm für den ersten DataFrame\n",
    "        hist_1 = go.Histogram(x=num_dwd_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "        fig.add_trace(hist_1, row=row, col=col)\n",
    "        \n",
    "        # Histogramm für den zweiten DataFrame\n",
    "        hist_2 = go.Histogram(x=num_dwd_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "        fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "    # Layout-Anpassungen\n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        width=1200,\n",
    "        title_text=f\"Histogramme der numerischen Spalten von DWD pes10 und NCEP pes10 von {year_to_filter}\",\n",
    "        showlegend=True,\n",
    "        margin=dict(t=50, b=20, l=20, r=20)\n",
    "    )\n",
    "\n",
    "    # Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "    fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "    pio.write_html(fig, f'historgramm_dwd_pes10_vs_ncep_pes10_von_{year_to_filter}.html', auto_open=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Liste der Jahre, die gefiltert werden sollen\n",
    "years_to_filter = [2021, 2022,2023,2024]\n",
    "\n",
    "# Schleife durch jedes Jahr\n",
    "for year_to_filter in years_to_filter:\n",
    "    df_dwd_demand_year = df_dwd_demand[df_dwd_demand['ref_datetime'].dt.year == year_to_filter]\n",
    "    df_ncep_demand_year = df_ncep_demand[df_ncep_demand['ref_datetime'].dt.year == year_to_filter]\n",
    "\n",
    "    # Numerische Spalten in beiden DataFrames auswählen\n",
    "    num_dwd_cols_1 = df_ncep_demand_year.select_dtypes(include=\"number\")\n",
    "    num_dwd_cols_2 = df_dwd_demand_year.select_dtypes(include=\"number\")\n",
    "\n",
    "    # Spalten definieren, die ausgeschlossen werden sollen\n",
    "    exclude_columns = [\"hours_after\"]\n",
    "\n",
    "    # Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "    common_columns = [col for col in num_dwd_cols_1.columns.intersection(num_dwd_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "    # Erzeuge das 2x3 Raster für die Subplots\n",
    "    fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "    # Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "    for i, column in enumerate(common_columns[:6]):\n",
    "        row = (i // 3) + 1\n",
    "        col = (i % 3) + 1\n",
    "\n",
    "        # Histogramm für den ersten DataFrame\n",
    "        hist_1 = go.Histogram(x=num_dwd_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "        fig.add_trace(hist_1, row=row, col=col)\n",
    "\n",
    "        # Histogramm für den zweiten DataFrame\n",
    "        hist_2 = go.Histogram(x=num_dwd_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "        fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "    # Layout-Anpassungen\n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        width=1200,\n",
    "        title_text=f\"Histogramme der numerischen Spalten von DWD demand und NCEP demand von {year_to_filter}\",\n",
    "        showlegend=True,\n",
    "        margin=dict(t=50, b=20, l=20, r=20)\n",
    "    )\n",
    "\n",
    "    # Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "    fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "    pio.write_html(fig, f'historgramm_dwd_demand_vs_ncep_demand_von_{year_to_filter}.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10_year.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dwd_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_filter = 2023\n",
    "df_dwd_demand_2024 = df_dwd_demand[df_dwd_demand['ref_datetime'].dt.year == year_to_filter]\n",
    "df_ncep_demand_2024 = df_ncep_demand[df_ncep_demand['ref_datetime'].dt.year == year_to_filter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Numerische Spalten in beiden DataFrames auswählen\n",
    "num_dwd_hornsea_cols_1 = df_dwd_demand_2024.select_dtypes(include=\"number\")\n",
    "num_dwd_hornsea_cols_2 = df_ncep_demand_2024.select_dtypes(include=\"number\")\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in num_dwd_hornsea_cols_1.columns.intersection(num_dwd_hornsea_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    hist_1 = go.Histogram(x=num_dwd_hornsea_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "    fig.add_trace(hist_1, row=row, col=col)\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    hist_2 = go.Histogram(x=num_dwd_hornsea_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "    fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    width=1200,\n",
    "    title_text=\"Histogramme der numerischen Spalten aus beiden DataFrames\",\n",
    "    showlegend=True,\n",
    "    margin=dict(t=50, b=20, l=20, r=20)\n",
    ")\n",
    "\n",
    "# Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "pio.write_html(fig, 'temp_demand.html', auto_open=True)  # Temporäre HTML-Datei erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dwd_demand_2024.groupby(['latitude','longitude'])['RelativeHumidity'].unique()\n",
    "\n",
    "unique_values_list = df_dwd_demand_2024.groupby(['latitude','longitude'])['RelativeHumidity'].agg(lambda x: list(x.unique()))\n",
    "\n",
    "print(unique_values_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_list = df_ncep_demand_2024.groupby(['latitude','longitude'])['RelativeHumidity'].agg(lambda x: list(x.unique()))\n",
    "\n",
    "print(unique_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remit-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit_filter_h3 = df_remit_filter.head()\n",
    "df_remit_filter_h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def export_styled_dataframes(dfs_dict, path='.'):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            print(f\"'{df_name}' ist kein DataFrame. Überspringe diesen Eintrag.\")\n",
    "            continue\n",
    "        \n",
    "        # Überprüfen, ob df_name 'df_remit_h3' ist und DataFrame aufteilen\n",
    "        if df_name == 'df_remit_h3':\n",
    "            # Angenommen, wir teilen df_remit_h3 in zwei Hälften\n",
    "            mid_index = len(df) // 2\n",
    "            df_part1 = df.iloc[:mid_index]  # Erste Hälfte\n",
    "            df_part2 = df.iloc[mid_index:]   # Zweite Hälfte\n",
    "\n",
    "            # Export der ersten Hälfte\n",
    "            export_single_dataframe(f\"{df_name}_part1\", df_part1, path)\n",
    "            # Export der zweiten Hälfte\n",
    "            export_single_dataframe(f\"{df_name}_part2\", df_part2, path)\n",
    "\n",
    "        else:\n",
    "            # Exportiere andere DataFrames\n",
    "            export_single_dataframe(df_name, df, path)\n",
    "\n",
    "def export_single_dataframe(df_name, df, path):\n",
    "    png_filename = f\"{df_name}.png\"\n",
    "    \n",
    "    styled_df = (df.style\n",
    "        .set_table_attributes('style=\"width: 100%; border-collapse: collapse;\"')\n",
    "        .set_table_styles(\n",
    "            [{'selector': 'thead th', \n",
    "              'props': [('background-color', 'darkblue'), \n",
    "                        ('color', 'white'), \n",
    "                        ('font-weight', 'bold'), \n",
    "                        ('border', '1px solid black')]}],\n",
    "            axis=0\n",
    "        )\n",
    "        .set_properties(**{\n",
    "            'border': '1px solid black',\n",
    "            'text-align': 'center',\n",
    "            'font-size': '14px'\n",
    "        })\n",
    "        .apply(lambda x: ['background-color: lightgrey' if i % 2 == 0 else '' for i in range(len(x))], axis=0)\n",
    "    )\n",
    "\n",
    "    float_columns = df.select_dtypes(include=['float']).columns\n",
    "    format_dict = {col: '{:.2f}' for col in float_columns}\n",
    "    styled_df = styled_df.format(format_dict)\n",
    "    \n",
    "    styled_df.set_table_styles(\n",
    "        [{'selector': 'td, th', \n",
    "          'props': [('border', '1px solid black')]}],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    png_full_path = f\"{path}/{png_filename}\"\n",
    "    \n",
    "    # Exportieren des DataFrames\n",
    "    dfi.export(styled_df, png_full_path, table_conversion='matplotlib')\n",
    "\n",
    "\n",
    "# Dictionary der DataFrames\n",
    "dataframes = {\n",
    "    'df_energy_h3': energy_h3,  \n",
    "    'df_dwd_hornsea_h3': df_hornsea_dwd_h3,  \n",
    "    'df_dwd_pes10_h3': df_dwd_pes10_h3,  \n",
    "    'df_dwd_demand_h3': df_dwd_demand_h3,  \n",
    "    'df_ncep_hornsea_h3': df_ncep_hornsea_h3,  \n",
    "    'df_ncep_pes10_h3': df_ncep_pes10_h3,  \n",
    "    'df_ncep_demand_h3': df_ncep_demand_h3,  \n",
    "    'df_remit_h3_1': df_remit_h3_1,\n",
    "    'df_remit_h3_2': df_remit_h3_2,\n",
    "}\n",
    "\n",
    "export_styled_dataframes(dataframes, path=r'...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remit_t = df_remit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funktion zum Extrahieren der Werte aus dem Dictionary\n",
    "def extract_values(outage_profile):\n",
    "    # Überprüfen, ob der Wert in der Spalte gültig ist\n",
    "    if isinstance(outage_profile, str):\n",
    "        # Suchen nach allen startTime, endTime und capacity Werten\n",
    "        start_times = re.findall(r\"'startTime': '(.*?)'\", outage_profile)\n",
    "        end_times = re.findall(r\"'endTime': '(.*?)'\", outage_profile)\n",
    "        capacities = re.findall(r\"'capacity': (\\d+)\", outage_profile)\n",
    "        \n",
    "        # Erstellen eines Dictionaries mit den Werten in nebeneinander liegenden Spalten\n",
    "        extracted_data = {}\n",
    "        for i in range(len(start_times)):\n",
    "            extracted_data[f'startTime_{i+1}'] = start_times[i]\n",
    "            extracted_data[f'endTime_{i+1}'] = end_times[i]\n",
    "            extracted_data[f'capacity_{i+1}'] = capacities[i]\n",
    "\n",
    "        return extracted_data\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Extrahieren der Werte aus der 'outageProfile'-Spalte\n",
    "extracted_values = df_remit_t['outageProfile'].apply(extract_values)\n",
    "\n",
    "# Erstellen eines neuen DataFrames aus den extrahierten Werten\n",
    "extracted_df = pd.DataFrame(extracted_values.tolist())\n",
    "\n",
    "# Umwandlung der Spalten in den entsprechenden Datentyp\n",
    "for col in extracted_df.columns:\n",
    "    if 'startTime' in col or 'endTime' in col:\n",
    "        extracted_df[col] = pd.to_datetime(extracted_df[col], errors='coerce')  # Fehlerhafte Einträge werden zu NaT\n",
    "    elif 'capacity' in col:\n",
    "        extracted_df[col] = extracted_df[col].astype(float)  # Oder int, je nach Bedarf\n",
    "\n",
    "display(extracted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Spalten in Datetime-Objekte\n",
    "for col in extracted_df.columns:\n",
    "    if 'startTime' in col or 'endTime' in col:\n",
    "        extracted_df[col] = pd.to_datetime(extracted_df[col])\n",
    "\n",
    "# Berechnung der Zeitdifferenzen für alle Zeitpaare\n",
    "for i in range(1, len(df.columns)//2 + 1):\n",
    "    start_col = f'startTime_{i}'\n",
    "    end_col = f'endTime_{i}'\n",
    "    time_col = f'Time_{i}'\n",
    "    \n",
    "    if start_col in extracted_df.columns and end_col in extracted_df.columns:\n",
    "        extracted_df[time_col] = extracted_df[end_col] - extracted_df[start_col]\n",
    "\n",
    "extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = extracted_df.min()\n",
    "max_values = extracted_df.max()\n",
    "mean_values = extracted_df.mean()\n",
    "median_values = extracted_df.median()\n",
    "\n",
    "min_max_df_extracted_df = pd.DataFrame({'Feature': min_values.index, 'Min': min_values.values, 'Max': max_values.values,'Mean': mean_values.values, 'Median': median_values.values})\n",
    "display(min_max_df_extracted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_ncep_pes10_year = df_ncep_pes10[df_ncep_pes10['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_dwd_pes10_year = df_dwd_pes10[df_dwd_pes10['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_dwd_pes10_year.columns.intersection(df_ncep_pes10_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    ax.hist(df_dwd_pes10_year[column], bins=100, alpha=0.5, label=f\"DWD: {column}\", color=\"blue\",stacked=True)\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    ax.hist(df_ncep_pes10_year[column], bins=100, alpha=0.5, label=f\"NCEP: {column}\", color=\"red\", )\n",
    "    \n",
    "    ax.set_title(column)\n",
    "\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD pes10 und NCEP pes10\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_pes10_vs_ncep_pes10.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_ncep_pes10_year = df_ncep_pes10[df_ncep_pes10['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_dwd_pes10_year = df_dwd_pes10[df_dwd_pes10['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_dwd_pes10_year.columns.intersection(df_ncep_pes10_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame (DWD)\n",
    "    ax.hist(df_dwd_pes10_year[column], bins=100, alpha=0.5, label=f\"DWD: {column}\", color=\"blue\")\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame (NCEP)\n",
    "    ax.hist(df_ncep_pes10_year[column], bins=100, alpha=0.5, label=f\"NCEP: {column}\", color=\"red\")\n",
    "    \n",
    "    ax.set_title(column)\n",
    "\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD pes10 und NCEP pes10\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_pes10_vs_ncep_pes10.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(19680801)\n",
    "\n",
    "n_bins = 10\n",
    "x = np.random.randn(1000, 3)\n",
    "\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "colors = ['red', 'tan', 'lime']\n",
    "ax0.hist(x, n_bins, density=True, histtype='bar', color=colors, label=colors)\n",
    "ax0.legend(prop={'size': 10})\n",
    "ax0.set_title('bars with legend')\n",
    "\n",
    "ax1.hist(x, n_bins, density=True, histtype='bar', stacked=True)\n",
    "ax1.set_title('stacked bar')\n",
    "\n",
    "ax2.hist(x, n_bins, histtype='step', stacked=True, fill=False)\n",
    "ax2.set_title('stack step (unfilled)')\n",
    "\n",
    "# Make a multiple-histogram of data-sets with different length.\n",
    "x_multi = [np.random.randn(n) for n in [10000, 5000, 2000]]\n",
    "ax3.hist(x_multi, n_bins, histtype='bar')\n",
    "ax3.set_title('different sample sizes')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_ncep_pes10_year = df_ncep_pes10[df_ncep_pes10['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_dwd_pes10_year = df_dwd_pes10[df_dwd_pes10['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_dwd_pes10_year.columns.intersection(df_ncep_pes10_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    ax.hist(df_dwd_pes10_year[column], bins=100, alpha=0.5, label=f\"DWD\", color=\"black\",density=True, stacked=True)\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    ax.hist(df_ncep_pes10_year[column], bins=100, alpha=0.5, label=f\"NCEP\", color=\"red\",density=True, stacked=True)\n",
    "    \n",
    "    ax.set_title(column)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD pes10 und NCEP pes10\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_pes10_vs_ncep_pes10_log.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_hornsea_ncep_year = df_hornsea_ncep[df_hornsea_ncep['reference_time'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_hornsea_dwd_year = df_hornsea_dwd[df_hornsea_dwd['reference_time'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_hornsea_dwd_year.columns.intersection(df_hornsea_ncep_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    ax.hist(df_hornsea_dwd_year[column], bins=100, alpha=0.5, label=f\"DWD: {column}\", color=\"blue\")\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    ax.hist(df_hornsea_ncep_year[column], bins=100, alpha=0.5, label=f\"NCEP: {column}\", color=\"red\")\n",
    "    \n",
    "    ax.set_title(column)\n",
    "\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD hornsea und NCEP hornsea\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_hornsea_vs_ncep_hornsea.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_hornsea_ncep_year = df_hornsea_ncep[df_hornsea_ncep['reference_time'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_hornsea_dwd_year = df_hornsea_dwd[df_hornsea_dwd['reference_time'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_hornsea_dwd_year.columns.intersection(df_hornsea_ncep_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    ax.hist(df_hornsea_dwd_year[column], bins=100, alpha=0.5, label=f\"DWD: {column}\", color=\"blue\")\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    ax.hist(df_hornsea_ncep_year[column], bins=100, alpha=0.5, label=f\"NCEP: {column}\", color=\"red\")\n",
    "    \n",
    "    ax.set_title(column)\n",
    "    ax.set_yscale('log')\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD hornsea und NCEP hornsea\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_hornsea_vs_ncep_hornsea_log.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_ncep_demand_year = df_ncep_demand[df_ncep_demand['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_dwd_demand_year = df_dwd_demand[df_dwd_demand['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_dwd_demand_year.columns.intersection(df_ncep_demand_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    ax.hist(df_dwd_demand_year[column], bins=100, alpha=0.5, label=f\"DWD: {column}\", color=\"blue\")\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    ax.hist(df_ncep_demand_year[column], bins=100, alpha=0.5, label=f\"NCEP: {column}\", color=\"red\")\n",
    "    \n",
    "    ax.set_title(column)\n",
    "\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD demand und NCEP demand\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_deamnd_vs_ncep_demand.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Jahre, die gefiltert werden sollen\n",
    "years_to_filter = [2021, 2022, 2023, 2024]\n",
    "\n",
    "# Schleife durch jedes Jahr und Halbjahr\n",
    "for year_to_filter in years_to_filter:\n",
    "    for half in [1, 2]:  # 1: Erstes Halbjahr, 2: Zweites Halbjahr\n",
    "        # Filter für das jeweilige Halbjahr anwenden\n",
    "        if half == 1:\n",
    "            df_dwd_pes10_half = df_dwd_pes10[(df_dwd_pes10['ref_datetime'].dt.year == year_to_filter) &\n",
    "                                             (df_dwd_pes10['ref_datetime'].dt.month <= 6)]\n",
    "            df_ncep_pes10_half = df_ncep_pes10[(df_ncep_pes10['ref_datetime'].dt.year == year_to_filter) &\n",
    "                                               (df_ncep_pes10['ref_datetime'].dt.month <= 6)]\n",
    "        else:\n",
    "            df_dwd_pes10_half = df_dwd_pes10[(df_dwd_pes10['ref_datetime'].dt.year == year_to_filter) &\n",
    "                                             (df_dwd_pes10['ref_datetime'].dt.month > 6)]\n",
    "            df_ncep_pes10_half = df_ncep_pes10[(df_ncep_pes10['ref_datetime'].dt.year == year_to_filter) &\n",
    "                                               (df_ncep_pes10['ref_datetime'].dt.month > 6)]\n",
    "\n",
    "        # Numerische Spalten in beiden DataFrames auswählen\n",
    "        num_dwd_cols_1 = df_dwd_pes10_half.select_dtypes(include=\"number\")\n",
    "        num_dwd_cols_2 = df_ncep_pes10_half.select_dtypes(include=\"number\")\n",
    "\n",
    "        # Spalten definieren, die ausgeschlossen werden sollen\n",
    "        exclude_columns = [\"hours_after\"]\n",
    "\n",
    "        # Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "        common_columns = [col for col in num_dwd_cols_1.columns.intersection(num_dwd_cols_2.columns) if col not in exclude_columns]\n",
    "\n",
    "        # Erzeuge das 2x3 Raster für die Subplots\n",
    "        fig = sp.make_subplots(rows=2, cols=3, subplot_titles=common_columns[:6], vertical_spacing=0.1)\n",
    "\n",
    "        # Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "        for i, column in enumerate(common_columns[:6]):\n",
    "            row = (i // 3) + 1\n",
    "            col = (i % 3) + 1\n",
    "            \n",
    "            # Histogramm für den ersten DataFrame\n",
    "            hist_1 = go.Histogram(x=num_dwd_cols_1[column], opacity=0.5, name=f\"DWD: {column}\", marker=dict(color=\"blue\"))\n",
    "            fig.add_trace(hist_1, row=row, col=col)\n",
    "            \n",
    "            # Histogramm für den zweiten DataFrame\n",
    "            hist_2 = go.Histogram(x=num_dwd_cols_2[column], opacity=0.5, name=f\"NCEP: {column}\", marker=dict(color=\"red\"))\n",
    "            fig.add_trace(hist_2, row=row, col=col)\n",
    "\n",
    "        # Layout-Anpassungen\n",
    "        fig.update_layout(\n",
    "            height=700,\n",
    "            width=1200,\n",
    "            title_text=f\"Histogramme der numerischen Spalten von DWD pes10 und NCEP pes10 von {year_to_filter}, Halbjahr {half}\",\n",
    "            showlegend=True,\n",
    "            margin=dict(t=50, b=20, l=20, r=20)\n",
    "        )\n",
    "\n",
    "        # Den Abstand zwischen der letzten Reihe und dem Bild verringern\n",
    "        fig.update_yaxes(title_standoff=0)\n",
    "\n",
    "        # Speichern des Diagramms als HTML-Datei\n",
    "        pio.write_html(fig, f'histogramm_dwd_pes10_vs_ncep_pes10_von_{year_to_filter}_halbjahr_{half}.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten filtern und numerische Spalten auswählen\n",
    "df_ncep_demand_year = df_ncep_demand[df_ncep_demand['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "df_dwd_demand_year = df_dwd_demand[df_dwd_demand['ref_datetime'].dt.year == year_to_filter].select_dtypes(include=\"number\")\n",
    "\n",
    "# Spalten definieren, die ausgeschlossen werden sollen\n",
    "exclude_columns = [\"hours_after\"]\n",
    "\n",
    "# Gemeinsame Spalten identifizieren und die auszuschließenden Spalten entfernen\n",
    "common_columns = [col for col in df_dwd_demand_year.columns.intersection(df_ncep_demand_year.columns) if col not in exclude_columns]\n",
    "\n",
    "# Erzeuge das 2x3 Raster für die Subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Schleife durch jede Spalte und füge Histogramme beider DataFrames hinzu\n",
    "for i, column in enumerate(common_columns[:6]):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogramm für den ersten DataFrame\n",
    "    ax.hist(df_dwd_demand_year[column], bins=100, alpha=0.5, label=f\"DWD: {column}\", color=\"blue\")\n",
    "    \n",
    "    # Histogramm für den zweiten DataFrame\n",
    "    ax.hist(df_ncep_demand_year[column], bins=100, alpha=0.5, label=f\"NCEP: {column}\", color=\"red\")\n",
    "    \n",
    "    ax.set_title(column)\n",
    "    ax.set_yscale('log')  # Setze die y-Achse auf logarithmische Skala\n",
    "\n",
    "# Legende außerhalb der Subplots platzieren\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout-Anpassungen\n",
    "fig.suptitle(f\"Histogramme der numerischen Spalten von DWD demand und NCEP demand\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Speichern der Grafik als PNG-Datei\n",
    "plt.savefig(f'histogramm_dwd_deamnd_vs_ncep_demand_log.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncep_pes10.groupby(['CloudCover']).count().head(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand['name'] = 'dwd'\n",
    "df_ncep_demand['name'] = 'ncep'\n",
    "\n",
    "test_demand = pd.concat([df_dwd_demand, df_ncep_demand, ])\n",
    "\n",
    "test_demand = test_demand.dropna()         \n",
    "           \n",
    "\n",
    "test_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand = test_demand.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_demand['name'].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand = test_demand.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand = test_demand.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_1000 = test_demand.head(1000)   # Erste 1000 Zeilen\n",
    "last_1000 = test_demand.tail(1000)     # Letzte 1000 Zeilen\n",
    "\n",
    "# Kombiniere die beiden DataFrames\n",
    "combined_df = pd.concat([first_1000, last_1000], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafiken vergleich der beiden Datensätz DWD vs NCEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_demand['name'] = 'dwd'\n",
    "df_ncep_demand['name'] = 'ncep'\n",
    "\n",
    "df_demand_ncep_hornsea = pd.concat([df_dwd_demand, df_ncep_demand])\n",
    "\n",
    "df_demand_ncep_hornsea = df_demand_ncep_hornsea.dropna()         \n",
    "           \n",
    "\n",
    "df_demand_ncep_hornsea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_1000 = test_demand.head(20000000)   # Erste 1000 Zeilen\n",
    "# last_1000 = test_demand.tail(20000000)     # Letzte 1000 Zeilen\n",
    "\n",
    "# Kombiniere die beiden DataFrames\n",
    "combined_df11 = test_demand.head(10813880)\n",
    "combined_df11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Verfügbare Spalten im DataFrame\n",
    "available_variables = combined_df11.columns.tolist()\n",
    "print(\"Available variables:\", available_variables)\n",
    "\n",
    "# Liste der Variablen und ihrer Namen, nur wenn sie im DataFrame existieren\n",
    "variables = ['point', 'RelativeHumidity', 'Temperature', 'TotalPrecipitation', 'WindDirection', 'WindSpeed']\n",
    "valid_variables = [var for var in variables if var in available_variables]\n",
    "\n",
    "# Dynamisches Einlesen der Gruppen\n",
    "names = combined_df11['name'].unique()\n",
    "\n",
    "# Setze den Stil für die Plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Berechne die Anzahl der benötigten Zeilen\n",
    "num_cols = 3  # Maximal 3 Spalten\n",
    "num_rows = (len(valid_variables) + num_cols - 1) // num_cols  # Runden auf die nächste ganze Zahl\n",
    "\n",
    "# Erstelle Subplots für die Distributionsdiagramme\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 6 * num_rows))\n",
    "axs = axs.flatten()  # Mache axs zu einem eindimensionalen Array\n",
    "\n",
    "# Definiere die Einheiten für jede Variable\n",
    "units = {\n",
    "    'point': '',\n",
    "    'RelativeHumidity': '%',\n",
    "    'Temperature': '°C',\n",
    "    'TotalPrecipitation': 'mm',\n",
    "    'WindDirection': '°',\n",
    "    'WindSpeed': 'm/s'\n",
    "}\n",
    "\n",
    "for i, variable in enumerate(valid_variables):\n",
    "    for name in names:\n",
    "        # Filtere die Daten für die jeweilige Gruppe\n",
    "        filtered_data = combined_df11[combined_df11['name'] == name]\n",
    "        \n",
    "        # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "        if filtered_data.empty:\n",
    "            # Ausgabe nur für Gruppen, die keine Daten haben\n",
    "            continue\n",
    "        \n",
    "        # Erstelle das Distributionsdiagramm für jede Gruppe\n",
    "        sns.kdeplot(\n",
    "            data=filtered_data,\n",
    "            x=variable,  \n",
    "            fill=True,\n",
    "            ax=axs[i],\n",
    "            alpha=0.5,\n",
    "            label=name  # Gruppennamen für die Legende hinzufügen\n",
    "        )\n",
    "    \n",
    "    # Titel und Achsenbeschriftungen hinzufügen\n",
    "    axs[i].set_title(f'Distribution of {variable}', fontsize=16)\n",
    "    \n",
    "    # Einheit zur x-Achsenbeschriftung hinzufügen, überspringe 'point'\n",
    "    if variable != 'point':\n",
    "        axs[i].set_xlabel(f'{variable} ({units[variable]})')\n",
    "    else:\n",
    "        axs[i].set_xlabel(variable)  # Nur den Namen ohne Einheit\n",
    "\n",
    "    axs[i].set_ylabel('Density')\n",
    "    axs[i].legend(title='Group')  # Legende hinzufügen\n",
    "\n",
    "    # Ganzzahlige Werte auf der Y-Achse einstellen\n",
    "    axs[i].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Leere Subplots ausblenden, falls weniger als num_rows*num_cols Variablen vorhanden sind\n",
    "for j in range(i + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Layout anpassen\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optional: Speichere das Diagramm oder zeige es an\n",
    "plt.savefig('distribution_plots_demand.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hornsea_dwd['name'] = 'dwd'\n",
    "df_hornsea_ncep['name'] = 'ncep'\n",
    "\n",
    "df_dwd_ncep_hornsea = pd.concat([df_hornsea_dwd, df_hornsea_ncep])\n",
    "\n",
    "df_dwd_ncep_hornsea = df_dwd_ncep_hornsea.dropna()         \n",
    "           \n",
    "\n",
    "df_dwd_ncep_hornsea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfügbare Spalten im DataFrame\n",
    "available_variables = df_dwd_ncep_hornsea.columns.tolist()\n",
    "print(\"Available variables:\", available_variables)\n",
    "\n",
    "# Liste der Variablen und ihrer Namen, nur wenn sie im DataFrame existieren\n",
    "variables = ['RelativeHumidity', 'Temperature', 'WindDirection', 'WindDirection:100', 'WindSpeed', 'WindSpeed:100']\n",
    "valid_variables = [var for var in variables if var in available_variables]\n",
    "\n",
    "# Dynamisches Einlesen der Gruppen\n",
    "names = df_dwd_ncep_hornsea['name'].unique()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Berechne die Anzahl der benötigten Zeilen\n",
    "num_cols = 3  # Maximal 3 Spalten\n",
    "num_rows = (len(valid_variables) + num_cols - 1) // num_cols  # Runden auf die nächste ganze Zahl\n",
    "\n",
    "# Erstelle Subplots für die Distributionsdiagramme\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 6 * num_rows))\n",
    "axs = axs.flatten()  # Mache axs zu einem eindimensionalen Array\n",
    "\n",
    "for i, variable in enumerate(valid_variables):\n",
    "    for name in names:\n",
    "        # Filtere die Daten für die jeweilige Gruppe\n",
    "        filtered_data = df_dwd_ncep_hornsea[df_dwd_ncep_hornsea['name'] == name]\n",
    "        \n",
    "        # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data available for group {name}.\")\n",
    "            continue\n",
    "        \n",
    "        # Erstelle das Distributionsdiagramm für jede Gruppe\n",
    "        sns.kdeplot(\n",
    "            data=filtered_data,\n",
    "            x=variable,  \n",
    "            fill=True,\n",
    "            ax=axs[i],\n",
    "            alpha=0.5,\n",
    "            label=name  # Gruppennamen für die Legende hinzufügen\n",
    "        )\n",
    "    \n",
    "    # Titel und Achsenbeschriftungen hinzufügen\n",
    "    axs[i].set_title(f'Distribution of {variable}', fontsize=16)\n",
    "    axs[i].set_xlabel(variable)\n",
    "    axs[i].set_ylabel('Density')\n",
    "    axs[i].legend(title='Group')  # Legende hinzufügen\n",
    "\n",
    "    # Ganzzahlige Werte auf der Y-Achse einstellen\n",
    "    axs[i].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Leere Subplots ausblenden, falls weniger als num_rows*num_cols Variablen vorhanden sind\n",
    "for j in range(i + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Layout anpassen\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optional: Speichere das Diagramm oder zeige es an\n",
    "plt.savefig('distribution_plots_hornsea.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Verfügbare Spalten im DataFrame\n",
    "available_variables = df_dwd_ncep_hornsea.columns.tolist()\n",
    "print(\"Available variables:\", available_variables)\n",
    "\n",
    "# Liste der Variablen und ihrer Namen, nur wenn sie im DataFrame existieren\n",
    "variables = ['RelativeHumidity', 'Temperature', 'WindDirection', 'WindDirection:100', 'WindSpeed', 'WindSpeed:100']\n",
    "valid_variables = [var for var in variables if var in available_variables]\n",
    "\n",
    "# Dynamisches Einlesen der Gruppen\n",
    "names = df_dwd_ncep_hornsea['name'].unique()\n",
    "\n",
    "# Setze den Stil für die Plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Berechne die Anzahl der benötigten Zeilen\n",
    "num_cols = 3  # Maximal 3 Spalten\n",
    "num_rows = (len(valid_variables) + num_cols - 1) // num_cols  # Runden auf die nächste ganze Zahl\n",
    "\n",
    "# Erstelle Subplots für die Distributionsdiagramme\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 6 * num_rows))\n",
    "axs = axs.flatten()  # Mache axs zu einem eindimensionalen Array\n",
    "\n",
    "# Definiere die Einheiten für jede Variable\n",
    "units = {\n",
    "    'RelativeHumidity': '%',\n",
    "    'Temperature': '°C',\n",
    "    'WindDirection': '°',\n",
    "    'WindDirection:100': '°',\n",
    "    'WindSpeed': 'm/s',\n",
    "    'WindSpeed:100': 'm/s'\n",
    "}\n",
    "\n",
    "for i, variable in enumerate(valid_variables):\n",
    "    for name in names:\n",
    "        # Filtere die Daten für die jeweilige Gruppe\n",
    "        filtered_data = df_dwd_ncep_hornsea[df_dwd_ncep_hornsea['name'] == name]\n",
    "        \n",
    "        # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data available for group {name}.\")\n",
    "            continue\n",
    "        \n",
    "        # Erstelle das Distributionsdiagramm für jede Gruppe\n",
    "        sns.kdeplot(\n",
    "            data=filtered_data,\n",
    "            x=variable,  \n",
    "            fill=True,\n",
    "            ax=axs[i],\n",
    "            alpha=0.5,\n",
    "            label=name  # Gruppennamen für die Legende hinzufügen\n",
    "        )\n",
    "    \n",
    "    # Titel und Achsenbeschriftungen hinzufügen\n",
    "    axs[i].set_title(f'Distribution of {variable}', fontsize=16)\n",
    "    axs[i].set_xlabel(f'{variable} ({units[variable]})')  # Einheit zur x-Achsenbeschriftung hinzufügen\n",
    "    axs[i].set_ylabel('Density')\n",
    "    axs[i].legend(title='Group')  # Legende hinzufügen\n",
    "\n",
    "    # Ganzzahlige Werte auf der Y-Achse einstellen\n",
    "    axs[i].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Maximalwert für die x-Achse einstellen\n",
    "    max_value = filtered_data[variable].max()\n",
    "    axs[i].set_xlim(0, max_value * 1.1)  # Setze das Maximum auf 10% über dem Maximalwert\n",
    "\n",
    "# Leere Subplots ausblenden, falls weniger als num_rows*num_cols Variablen vorhanden sind\n",
    "for j in range(i + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Layout anpassen\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optional: Speichere das Diagramm oder zeige es an\n",
    "plt.savefig('distribution_plots_hornsea.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pes 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dwd_pes10['name'] = 'dwd'\n",
    "df_ncep_pes10['name'] = 'ncep'\n",
    "\n",
    "df_dwd_ncep_pes10 = pd.concat([df_dwd_pes10, df_ncep_pes10])\n",
    "\n",
    "df_dwd_ncep_pes10 = df_dwd_ncep_pes10.dropna()         \n",
    "           \n",
    "df_dwd_ncep_pes10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfügbare Spalten im DataFrame\n",
    "available_variables = df_dwd_ncep_pes10.columns.tolist()\n",
    "print(\"Available variables:\", available_variables)\n",
    "\n",
    "# Liste der Variablen und ihrer Namen, nur wenn sie im DataFrame existieren\n",
    "variables = ['point', 'CloudCover', 'SolarDownwardRadiation', 'Temperature', 'latitude', 'longitude']\n",
    "valid_variables = [var for var in variables if var in available_variables]\n",
    "\n",
    "# Dynamisches Einlesen der Gruppen\n",
    "names = df_dwd_ncep_pes10['name'].unique()\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Berechne die Anzahl der benötigten Zeilen\n",
    "num_cols = 3  # Maximal 3 Spalten\n",
    "num_rows = (len(valid_variables) + num_cols - 1) // num_cols  # Runden auf die nächste ganze Zahl\n",
    "\n",
    "# Erstelle Subplots für die Distributionsdiagramme\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 6 * num_rows))\n",
    "axs = axs.flatten()  # Mache axs zu einem eindimensionalen Array\n",
    "\n",
    "# Definiere die Einheiten für jede Variable\n",
    "units = {\n",
    "    'point': '',\n",
    "    'CloudCover': '%',\n",
    "    'SolarDownwardRadiation': 'W/m²',\n",
    "    'Temperature': '°C',\n",
    "    'latitude': '',\n",
    "    'longitude': ''\n",
    "}\n",
    "\n",
    "for i, variable in enumerate(valid_variables):\n",
    "    for name in names:\n",
    "        # Filtere die Daten für die jeweilige Gruppe\n",
    "        filtered_data = df_dwd_ncep_pes10[df_dwd_ncep_pes10['name'] == name]\n",
    "        \n",
    "        # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "        if filtered_data.empty:\n",
    "            continue  # Keine Ausgabe, wenn keine Daten vorhanden sind\n",
    "        \n",
    "        # Erstelle das Distributionsdiagramm für jede Gruppe\n",
    "        sns.kdeplot(\n",
    "            data=filtered_data,\n",
    "            x=variable,  \n",
    "            fill=True,\n",
    "            ax=axs[i],\n",
    "            alpha=0.5,\n",
    "            label=name  # Gruppennamen für die Legende hinzufügen\n",
    "        )\n",
    "    \n",
    "    # Titel und Achsenbeschriftungen hinzufügen\n",
    "    axs[i].set_title(f'Distribution of {variable}', fontsize=16)\n",
    "    \n",
    "    # Einheit zur x-Achsenbeschriftung hinzufügen, überspringe 'point', 'latitude' und 'longitude'\n",
    "    if variable not in ['point', 'latitude', 'longitude']:\n",
    "        axs[i].set_xlabel(f'{variable} ({units[variable]})')\n",
    "    else:\n",
    "        axs[i].set_xlabel(variable)  # Nur den Namen ohne Einheit\n",
    "\n",
    "    axs[i].set_ylabel('Density')\n",
    "    axs[i].legend(title='Group')  # Legende hinzufügen\n",
    "\n",
    "    # Ganzzahlige Werte auf der Y-Achse einstellen\n",
    "    axs[i].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "# Leere Subplots ausblenden, falls weniger als num_rows*num_cols Variablen vorhanden sind\n",
    "for j in range(i + 1, num_rows * num_cols):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "# Layout anpassen\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optional: Speichere das Diagramm oder zeige es an\n",
    "plt.savefig('distribution_plots_pes10.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_histograms(combined_df11, variables, group_col='name', max_cols=3):\n",
    "    # Verfügbare Spalten im DataFrame\n",
    "    available_variables = combined_df11.columns.tolist()\n",
    "    print(\"Available variables:\", available_variables)\n",
    "\n",
    "    # Liste der Variablen und ihrer Namen, nur wenn sie im DataFrame existieren\n",
    "    valid_variables = [var for var in variables if var in available_variables]\n",
    "\n",
    "    # Dynamisches Einlesen der Gruppen\n",
    "    names = combined_df11[group_col].unique()\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Berechne die Anzahl der benötigten Zeilen\n",
    "    num_rows = (len(valid_variables) + max_cols - 1) // max_cols  # Runden auf die nächste ganze Zahl\n",
    "\n",
    "    # Erstelle Subplots für die Histogramme\n",
    "    fig, axs = plt.subplots(num_rows, max_cols, figsize=(18, 6 * num_rows))\n",
    "    axs = axs.flatten()  # Mache axs zu einem eindimensionalen Array\n",
    "\n",
    "    for i, variable in enumerate(valid_variables):\n",
    "        for name in names:\n",
    "            # Filtere die Daten für die jeweilige Gruppe\n",
    "            filtered_data = combined_df11[combined_df11[group_col] == name]\n",
    "            \n",
    "            # Überprüfen, ob gefilterte Daten vorhanden sind\n",
    "            if filtered_data.empty:\n",
    "                print(f\"No data available for group {name}.\")\n",
    "                continue\n",
    "            \n",
    "            # Erstelle ein Histogramm für jede Gruppe\n",
    "            sns.histplot(\n",
    "                data=filtered_data,\n",
    "                x=variable,\n",
    "                ax=axs[i],\n",
    "                bins=30,  # Anzahl der Bins kann angepasst werden\n",
    "                stat='count',  # Zähle die Anzahl der Beobachtungen in jedem Bin\n",
    "                alpha=0.5,\n",
    "                label=name,  # Gruppennamen für die Legende hinzufügen\n",
    "                kde=False  # Dichtefunktion ausblenden\n",
    "            )\n",
    "        \n",
    "        # Titel und Achsenbeschriftungen hinzufügen\n",
    "        axs[i].set_title(f'Distribution of {variable}', fontsize=16)\n",
    "        axs[i].set_xlabel(variable)\n",
    "        axs[i].set_ylabel('Count')  # y-Achsenbezeichnung auf 'Count' ändern\n",
    "        axs[i].legend(title='Group')  # Legende hinzufügen\n",
    "\n",
    "        # Ganzzahlige Werte auf der Y-Achse einstellen\n",
    "        axs[i].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Leere Subplots ausblenden, falls weniger als num_rows*num_cols Variablen vorhanden sind\n",
    "    for j in range(i + 1, num_rows * max_cols):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Layout anpassen\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Optional: Speichere das Diagramm oder zeige es an\n",
    "    plt.savefig('histogram_plots_demand.png')\n",
    "    plt.show()\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "variables = ['point', 'RelativeHumidity', 'Temperature', 'TotalPrecipitation', 'WindDirection', 'WindSpeed']\n",
    "plot_histograms(combined_df11, variables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
