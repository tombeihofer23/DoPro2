{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Tom\\\\HKA\\\\7_Semester\\\\Dom√§nenprojekt_2\\\\DoPro'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Tom\\HKA\\7_Semester\\Dom√§nenprojekt_2\\DoPro\\.env\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Tom\\HKA\\7_Semester\\Dom√§nenprojekt_2\\DoPro\\.env\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=42378371-53a5-45fd-b93b-8bcf72a0dafd&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=9cc217f904b651bc66a09839b5ffeaf044bbdff845cc2c38404532f9da69bd71\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as tombeihofer23\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as tombeihofer23\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"tombeihofer23/DoPro2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"tombeihofer23/DoPro2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository tombeihofer23/DoPro2 initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository tombeihofer23/DoPro2 initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/11 15:12:11 INFO mlflow.tracking._tracking_service.client: üèÉ View run youthful-ram-521 at: https://dagshub.com/tombeihofer23/DoPro2.mlflow/#/experiments/0/runs/1a7b5c778f4b4f31985fc8f6b7d79311.\n",
      "2024/10/11 15:12:11 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/tombeihofer23/DoPro2.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='tombeihofer23', repo_name='DoPro2', mlflow=True)\n",
    "\n",
    "import mlflow\n",
    "with mlflow.start_run():\n",
    "  mlflow.log_param('parameter name', 'value')\n",
    "  mlflow.log_metric('metric name', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.regression.quantile_regression import QuantRegResults\n",
    "from statsmodels.base.model import Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation config entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Entity-Class for data evaluation config params.\"\"\"\n",
    "\n",
    "    path_to_models: Path\n",
    "    \"\"\"Directory in which models are stored.\"\"\"\n",
    "\n",
    "    training_data_path: Path\n",
    "    \"\"\"Directory where training data is stored.\"\"\"\n",
    "\n",
    "    all_params: dict\n",
    "    \"\"\"Model parameters.\"\"\"\n",
    "\n",
    "    mlflow_uri: str\n",
    "    \"\"\"URL to MLFlow dashboard.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dopro2_HEFTcom_challenge.constants import PARAMS_FILE_PATH, CONFIG_FILE_PATH\n",
    "import yaml\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"Class to manage all configurations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: Path = CONFIG_FILE_PATH,\n",
    "        params_filepath: Path = PARAMS_FILE_PATH\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for ConfigurationManager Class.\n",
    "        Creates artifacts folder.\n",
    "\n",
    "        :param config_filepath: Path to config.yaml file\n",
    "        :param params_filepath: Path to params.yaml file\n",
    "\n",
    "        \"\"\"\n",
    "        with config_filepath.open(\"r\") as f:\n",
    "            self.config: dict = yaml.safe_load(f)\n",
    "\n",
    "        with params_filepath.open(\"r\") as f:\n",
    "            self.params: dict = yaml.safe_load(f)\n",
    "\n",
    "        os.makedirs(self.config[\"artifacts_root\"], exist_ok=True)\n",
    "        logger.info(\"created directory at: {}\", self.config[\"artifacts_root\"])\n",
    "\n",
    "    def get_evalution_config(self) -> EvaluationConfig:\n",
    "        \"\"\"\n",
    "        Get all config params and create folder in artifacts dir.\n",
    "        \n",
    "        :return: values from config.yaml\n",
    "        :rtype: EvaluationConfig\n",
    "        \"\"\"\n",
    "\n",
    "        config = self.config[\"evaluation\"]\n",
    "\n",
    "        evaluation_config = EvaluationConfig(\n",
    "            path_to_models=config[\"path_to_models\"],\n",
    "            training_data_path=config[\"training_data_path\"],\n",
    "            all_params=self.params,\n",
    "            mlflow_uri=config[\"mlflow_uri\"]\n",
    "        )\n",
    "\n",
    "        return evaluation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\"Class to evaluate the model.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EvaluationConfig) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for Evaluation class.\n",
    "\n",
    "        :param config: config values from config.yaml\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    @staticmethod\n",
    "    def load_models(path: Path) -> list[Results]:\n",
    "        model_files = Path(path).glob(\"*.pickle\")\n",
    "        models = []\n",
    "        for file in model_files:\n",
    "            models.append(QuantRegResults.load(file))\n",
    "        return models\n",
    "    \n",
    "    @staticmethod\n",
    "    def pinball_score(df):\n",
    "        def pinball(y,q,alpha):\n",
    "            return (y-q)*alpha*(y>=q) + (q-y)*(1-alpha)*(y<q)\n",
    "        \n",
    "        score = list()\n",
    "        for qu in range(10,100,10):\n",
    "            score.append(pinball(y=df[\"total_generation_MWh\"],\n",
    "                q=df[f\"q{qu}\"],\n",
    "                alpha=qu/100).mean())\n",
    "        return sum(score)/len(score)\n",
    "    \n",
    "    def make_predictions(self):\n",
    "        self.models = self.load_models(self.config.path_to_models)\n",
    "        test_data = pd.read_parquet(self.config.training_data_path).iloc[400000:] # nur jetzt zum testen mit iloc\n",
    "        logger.info(\"Start making predictions on the trained models.\")\n",
    "        for i, model in enumerate(self.models):\n",
    "            test_data = test_data.copy()\n",
    "            test_data[f\"q{(i+1)*10}\"] = model.predict(test_data)\n",
    "            test_data.loc[test_data[f\"q{(i+1)*10}\"] < 0, f\"q{(i+1)*10}\"] = 0\n",
    "        self.predictions = test_data[[\"total_generation_MWh\", \n",
    "                                      \"q10\", \"q20\", \"q30\", \"q40\", \"q50\", \n",
    "                                      \"q60\", \"q70\", \"q80\", \"q90\"]]\n",
    "        logger.info(\"Made predictions on the trained models.\")\n",
    "        \n",
    "    def evaluation(self):\n",
    "        logger.info(\"Calculate the pinball score on the predictions.\")\n",
    "        self.score = self.pinball_score(self.predictions)\n",
    "        with open(\"score.txt\", \"w\") as f:\n",
    "            f.write(f\"Pinball Score: {self.score}\")\n",
    "        logger.info(\"Score file saved at: score.txt\")\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics(\n",
    "                {\"pinball score\": self.score}\n",
    "            )\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                for i, model in enumerate(self.models):\n",
    "                    mlflow.statsmodels.log_model(model, \"model\", registered_model_name=f\"q{(i+1)*10}\")\n",
    "            else:\n",
    "                for i, model in enumerate(self.models):\n",
    "                    mlflow.statsmodels.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-11 16:51:11.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mcreated directory at: artifacts\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-11 16:51:26.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmake_predictions\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mStart making predictions on the trained models.\u001b[0m\n",
      "\u001b[32m2024-10-11 16:51:27.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmake_predictions\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mMade predictions on the trained models.\u001b[0m\n",
      "\u001b[32m2024-10-11 16:51:27.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluation\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mCalculate the pinball score on the predictions.\u001b[0m\n",
      "\u001b[32m2024-10-11 16:51:27.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mevaluation\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mScore file saved at: score.txt\u001b[0m\n",
      "2024/10/11 16:51:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q10'.\n",
      "2024/10/11 16:54:32 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q10, version 1\n",
      "Created version '1' of model 'q10'.\n",
      "2024/10/11 16:54:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q20'.\n",
      "2024/10/11 16:57:20 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q20, version 1\n",
      "Created version '1' of model 'q20'.\n",
      "2024/10/11 16:57:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q30'.\n",
      "2024/10/11 17:00:06 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q30, version 1\n",
      "Created version '1' of model 'q30'.\n",
      "2024/10/11 17:00:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q40'.\n",
      "2024/10/11 17:02:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q40, version 1\n",
      "Created version '1' of model 'q40'.\n",
      "2024/10/11 17:03:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q50'.\n",
      "2024/10/11 17:06:39 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q50, version 1\n",
      "Created version '1' of model 'q50'.\n",
      "2024/10/11 17:06:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q60'.\n",
      "2024/10/11 17:13:06 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q60, version 1\n",
      "Created version '1' of model 'q60'.\n",
      "2024/10/11 17:13:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q70'.\n",
      "2024/10/11 17:15:58 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q70, version 1\n",
      "Created version '1' of model 'q70'.\n",
      "2024/10/11 17:16:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q80'.\n",
      "2024/10/11 17:18:46 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q80, version 1\n",
      "Created version '1' of model 'q80'.\n",
      "2024/10/11 17:18:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'q90'.\n",
      "2024/10/11 17:21:32 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: q90, version 1\n",
      "Created version '1' of model 'q90'.\n",
      "2024/10/11 17:21:32 INFO mlflow.tracking._tracking_service.client: üèÉ View run agreeable-ant-981 at: https://dagshub.com/tombeihofer23/DoPro2.mlflow/#/experiments/0/runs/82f735e47c7f42ee81c18c76b43efa16.\n",
      "2024/10/11 17:21:32 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://dagshub.com/tombeihofer23/DoPro2.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evalution_config()\n",
    "    evaluation = Evaluation(config=eval_config)\n",
    "    evaluation.make_predictions()\n",
    "    evaluation.evaluation()\n",
    "    evaluation.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
